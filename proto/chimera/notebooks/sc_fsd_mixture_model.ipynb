{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolving single-cell family size distribution into chimeric and real components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "notebook_path = os.path.abspath('')\n",
    "sources_path = os.path.abspath(os.path.join(notebook_path, '..', 'sources'))\n",
    "sys.path.insert(0, sources_path)\n",
    "\n",
    "import time\n",
    "import matplotlib.pylab as plt\n",
    "from commons import *\n",
    "from sc_fingerprint import SingleCellFingerprint, SingleCellFingerprintDataStore\n",
    "from sc_fsd_mixture_model import GeneralNegativeBinomialMixtureFamilySizeDistributionCodec, \\\n",
    "    SingleCellFamilySizeModel, DownsamplingRegularizedELBOLoss, get_expression_map\n",
    "\n",
    "import pyro\n",
    "from pyro.infer import Trace_ELBO, SVI\n",
    "from pyro.optim import Adam\n",
    "from pyro_extras import checkpoint_model, load_latest_checkpoint\n",
    "from pyro_extras import ZeroInflatedNegativeBinomial\n",
    "\n",
    "import torch\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import logging\n",
    "from collections import Counter\n",
    "import pickle \n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs, outputs, and model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'pbmc4k_ss_rate_0.25'\n",
    "# sc_fingerprint_path = '/home/jupyter/data/10x/pbmc4k_ss_rate_0.25_sc_fingerprint.pkl'\n",
    "\n",
    "dataset_name = 'pbmc4k'\n",
    "sc_fingerprint_path = '/home/jupyter/data/10x/pbmc4k_sc_fingerprint.pkl'\n",
    "\n",
    "zinb_fitter_kwargs = {\n",
    "    'lr': 0.5,\n",
    "    'max_iters': 10_000,\n",
    "    'p_zero_l1_reg': 0.001,\n",
    "    'outlier_stringency': 5.0,\n",
    "    'max_zinb_p_zero': 0.9995,\n",
    "    'min_zinb_p_zero': 0.0005,\n",
    "    'min_nb_phi': 0.01,\n",
    "    'max_nb_phi': 0.95\n",
    "}\n",
    "\n",
    "\n",
    "# generate a gene expression ladder for testing the full dynamic range\n",
    "all_genes_default_filter = np.load(\n",
    "    '/home/jupyter/data/10x/out/pbmc4k__alpha_0.0__beta_1.0__default_gene_filters__zinb/gene_indices.npy')\n",
    "gene_expr_ladder_idx_list = all_genes_default_filter[::100]\n",
    "\n",
    "# Instantiate the fingerprint datastore\n",
    "sc_fingerprint = SingleCellFingerprint.load(sc_fingerprint_path)\n",
    "sc_fingerprint_datastore = SingleCellFingerprintDataStore(\n",
    "    sc_fingerprint,\n",
    "    gene_idx_list=gene_expr_ladder_idx_list,\n",
    "    n_gene_groups=10,\n",
    "    zinb_fitter_kwargs=zinb_fitter_kwargs,\n",
    "    max_estimated_chimera_family_size=0)\n",
    "\n",
    "\n",
    "# # load fingerprint and instantiate the data-store\n",
    "# sc_fingerprint = SingleCellFingerprint.load(sc_fingerprint_path)\n",
    "# sc_fingerprint = sc_fingerprint.filter_genes()\n",
    "\n",
    "# # Instantiate the fingerprint datastore\n",
    "# sc_fingerprint_datastore = SingleCellFingerprintDataStore(\n",
    "#     sc_fingerprint,\n",
    "#     top_k_genes=sc_fingerprint.num_genes,\n",
    "#     n_gene_groups=100,\n",
    "#     zinb_fitter_kwargs=zinb_fitter_kwargs,\n",
    "#     max_estimated_chimera_family_size=0)\n",
    "\n",
    "\n",
    "# sc_fingerprint = SingleCellFingerprint.load(sc_fingerprint_path)\n",
    "# sc_fingerprint_datastore = SingleCellFingerprintDataStore(\n",
    "#     sc_fingerprint, top_k_genes=50, n_gene_groups=10, zinb_fitter_kwargs=zinb_fitter_kwargs)\n",
    "\n",
    "# # generate top 200 genes\n",
    "# all_genes_default_filter = np.load(\n",
    "#     '/home/jupyter/data/10x/out/pbmc4k__alpha_0.0__beta_1.0__default_gene_filters__zinb/gene_indices.npy')\n",
    "# gene_expr_top_200_idx_list = all_genes_default_filter[:200]\n",
    "\n",
    "# # Instantiate the fingerprint datastore\n",
    "# sc_fingerprint = SingleCellFingerprint.load(sc_fingerprint_path)\n",
    "# sc_fingerprint_datastore = SingleCellFingerprintDataStore(\n",
    "#     sc_fingerprint,\n",
    "#     gene_idx_list=gene_expr_top_200_idx_list,\n",
    "#     n_gene_groups=10,\n",
    "#     zinb_fitter_kwargs=zinb_fitter_kwargs,\n",
    "#     max_estimated_chimera_family_size=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, training, and regularization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 10_000\n",
    "\n",
    "mb_genes_per_gene_group = 10\n",
    "mb_expressing_cells_per_gene = 15\n",
    "mb_silent_cells_per_gene = 1\n",
    "\n",
    "training_e_lo_sum_width = 10\n",
    "training_e_hi_sum_width = 10\n",
    "\n",
    "training_e_lo_log_prob_prefactor = 1.0\n",
    "training_e_hi_log_prob_prefactor = 1.0\n",
    "training_e_obs_log_prob_prefactor = 1.0\n",
    "training_fingerprint_obs_log_prob_prefactor = 1.0\n",
    "\n",
    "disable_downsampling_regularization = True\n",
    "downsampling_regularization_strength = 1.0\n",
    "min_downsampling_rate = 0.5\n",
    "max_downsampling_rate = 1.0\n",
    "mb_genes_per_gene_group_reg = 1\n",
    "mb_expressing_cells_per_gene_reg = 2\n",
    "mb_silent_cells_per_gene_reg = 1\n",
    "\n",
    "\n",
    "\n",
    "init_params_dict = {\n",
    "    'chimera.alpha_c': 0.0,\n",
    "    'chimera.beta_c': 0.5,\n",
    "    'expr.phi_e_lo': 0.2,\n",
    "    'fsd.gmm_num_components': 5,\n",
    "    'fsd.gmm_dirichlet_concentration': 1.0,\n",
    "    'fsd.gmm_min_weight_per_component': 1e-3,\n",
    "    'fsd.gmm_init_xi_scale': 1.5,\n",
    "    'fsd.gmm_min_xi_scale': 1.0,\n",
    "    'fsd.gmm_init_components_perplexity': 0.5,\n",
    "    'fsd.enable_fsd_w_dirichlet_reg': True,\n",
    "    'fsd.enable_gmm_scale_optimization': True,\n",
    "    'fsd.w_lo_dirichlet_reg_strength': 1.0,\n",
    "    'fsd.w_hi_dirichlet_reg_strength': 1.0,\n",
    "    'fsd.w_lo_dirichlet_concentration': 0.001,\n",
    "    'fsd.w_hi_dirichlet_concentration': 0.001,\n",
    "    'fsd.xi_posterior_min_scale': 0.0,\n",
    "    'chimera.enable_hyperparameter_optimization': False}\n",
    "\n",
    "\n",
    "fsd_init_params_dict = {\n",
    "    'fsd_init.min_mu_lo': 0.02,\n",
    "    'fsd_init.min_mu_hi': 0.2,\n",
    "    'fsd_init.max_phi_lo': 1.5,\n",
    "    'fsd_init.max_phi_hi': 0.5,\n",
    "    'fsd_init.mu_decay': 0.75,\n",
    "    'fsd_init.w_decay': 0.2,\n",
    "    'fsd_init.mu_lo_to_mu_hi_ratio': 0.05}\n",
    "\n",
    "\n",
    "\n",
    "model_constraint_params_dict = {\n",
    "    # phi_lo is softly pinned to 1.0 (to decrease the parameter complexity of chimeras)\n",
    "    'phi_lo_comps': {\n",
    "        'pin_value': 1.0,\n",
    "        'pin_strength': 1e8,\n",
    "        'pin_exponent': 2},\n",
    "    \n",
    "    # no component of p_hi is allowed to be too over-dispersed\n",
    "    'phi_hi_comps': {\n",
    "        'upper_bound_value': 0.5,\n",
    "        'upper_bound_width': 0.1,\n",
    "        'upper_bound_strength': 1e4,\n",
    "        'upper_bound_exponent': 2},\n",
    "\n",
    "    # no component of p_lo can have too small or too large of a mean family size compared to the empirical mean\n",
    "    'mu_lo_comps_to_mu_empirical_ratio': {\n",
    "        'lower_bound_value': 0.001,\n",
    "        'lower_bound_width': 0.001,\n",
    "        'lower_bound_strength': 1e4,\n",
    "        'lower_bound_exponent': 2,\n",
    "        'upper_bound_value': 0.1,\n",
    "        'upper_bound_width': 0.01,\n",
    "        'upper_bound_strength': 1e4,\n",
    "        'upper_bound_exponent': 2},\n",
    "    \n",
    "    # no component of p_hi can have too small or too large of a mean family size compared to the empirical mean\n",
    "    'mu_hi_comps_to_mu_empirical_ratio': {\n",
    "        'lower_bound_value': 0.3,\n",
    "        'lower_bound_width': 0.1,\n",
    "        'lower_bound_strength': 1e4,\n",
    "        'lower_bound_exponent': 2,\n",
    "        'upper_bound_value': 5.0,\n",
    "        'upper_bound_width': 1.0,\n",
    "        'upper_bound_strength': 1e4,\n",
    "        'upper_bound_exponent': 2},\n",
    "    \n",
    "    # the ratio of observed chimeric to real molecules can not exceed a certain value\n",
    "    'p_obs_lo_to_p_obs_hi_ratio': {\n",
    "        'upper_bound_value': 0.2,\n",
    "        'upper_bound_width': 0.05,\n",
    "        'upper_bound_strength': 1e4,\n",
    "        'upper_bound_exponent': 2},\n",
    "\n",
    "    # the component weights of p_hi cannot reach below a certain value (for stability of Dirichlet prior)\n",
    "    'w_hi_comps': {\n",
    "        'lower_bound_value': 0.01,\n",
    "        'lower_bound_width': 0.005,\n",
    "        'lower_bound_strength': 1e6,\n",
    "        'lower_bound_exponent': 1},\n",
    "    \n",
    "    # the component weights of p_hi cannot reach below a certain value (for stability of Dirichlet prior)\n",
    "    'w_lo_comps': {\n",
    "        'lower_bound_value': 0.01,\n",
    "        'lower_bound_width': 0.005,\n",
    "        'lower_bound_strength': 1e6,\n",
    "        'lower_bound_exponent': 1},\n",
    "\n",
    "    # make sure that phi_e_hi is not too small or too large (to prevent over/under-flows)\n",
    "    'phi_e_hi_batch': {\n",
    "        'lower_bound_value': 0.001,\n",
    "        'lower_bound_width': 0.001,\n",
    "        'lower_bound_strength': 1e4,\n",
    "        'lower_bound_exponent': 2,\n",
    "        'upper_bound_value': 5.0,\n",
    "        'upper_bound_width': 1.0,\n",
    "        'upper_bound_strength': 1e4,\n",
    "        'upper_bound_exponent': 2},\n",
    "\n",
    "    # make sure that logit_p_zero_e_hi_batch is not too small or too large (to prevent over/under-flows)\n",
    "    'logit_p_zero_e_hi_batch': {\n",
    "        'lower_bound_value': -8.0,\n",
    "        'lower_bound_width': 1.0,\n",
    "        'lower_bound_strength': 1e4,\n",
    "        'lower_bound_exponent': 2,\n",
    "        'upper_bound_value': 8.0,\n",
    "        'upper_bound_width': 1.0,\n",
    "        'upper_bound_strength': 1e4,\n",
    "        'upper_bound_exponent': 2},\n",
    "}\n",
    "\n",
    "\n",
    "def adam_args(module_name, param_name):\n",
    "    slow_lr = 2e-3\n",
    "    slow_params = {\n",
    "        'alpha_c', 'beta_c',\n",
    "        'mu_e_lo', 'phi_e_lo',\n",
    "        'mu_e_hi', 'phi_e_hi', 'logit_p_zero_e_hi',\n",
    "        'fsd_xi_prior_weights_map'}\n",
    "    \n",
    "    fast_lr = 5e-3\n",
    "    fast_params = {}\n",
    "    \n",
    "    default_lr = 5e-3\n",
    "    \n",
    "    if param_name in slow_params:\n",
    "        lr = slow_lr\n",
    "    elif param_name in fast_params:\n",
    "        lr = fast_lr\n",
    "    else:\n",
    "        lr = default_lr\n",
    "    \n",
    "    return {\"lr\": lr, \"eps\": 1e-4, 'betas': (0.95, 0.999)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jupyter/data/10x/out'\n",
    "suffix = \"expr_small_ladder_test__poisson_proper\"\n",
    "# suffix = \"default_gene_filters__rc_poisson_proper\"\n",
    "# suffix = \"top_200_genes__poisson_e_obs__zinb_heavy_reg_gmm__new\"\n",
    "# suffix = \"top_50_genes__zinb_heavy_reg_gmm\"\n",
    "\n",
    "if init_params_dict['chimera.enable_hyperparameter_optimization']:\n",
    "    output_path = os.path.join(\n",
    "        root,\n",
    "        f\"{dataset_name}_\" +\n",
    "        f\"_variable_chimera_hyperparameters_\" +\n",
    "        f\"_{suffix}\")\n",
    "else:\n",
    "    output_path = os.path.join(\n",
    "        root,\n",
    "        f\"{dataset_name}_\" +\n",
    "        f\"_alpha_{init_params_dict['chimera.alpha_c']:.1f}_\" +\n",
    "        f\"_beta_{init_params_dict['chimera.beta_c']:.1f}_\" +\n",
    "        f\"_{suffix}\")\n",
    "\n",
    "checkpoint_path = os.path.join(output_path, \"checkpoints\")\n",
    "\n",
    "try:\n",
    "    print(output_path)\n",
    "    print(checkpoint_path)\n",
    "    os.mkdir(output_path)\n",
    "    os.mkdir(checkpoint_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "e_lo_map_output_path = os.path.join(output_path, \"e_lo_map.npy\")\n",
    "e_lo_mean_output_path = os.path.join(output_path, \"e_lo_mean.npy\")\n",
    "e_lo_var_output_path = os.path.join(output_path, \"e_lo_var.npy\")\n",
    "e_lo_ci_lower_output_path = os.path.join(output_path, \"e_lo_ci_lower.npy\")\n",
    "e_lo_ci_upper_output_path = os.path.join(output_path, \"e_lo_ci_upper.npy\")\n",
    "\n",
    "e_hi_map_output_path = os.path.join(output_path, \"e_hi_map.npy\")\n",
    "e_hi_mean_output_path = os.path.join(output_path, \"e_hi_mean.npy\")\n",
    "e_hi_var_output_path = os.path.join(output_path, \"e_hi_var.npy\")\n",
    "e_hi_ci_lower_output_path = os.path.join(output_path, \"e_hi_ci_lower.npy\")\n",
    "e_hi_ci_upper_output_path = os.path.join(output_path, \"e_hi_ci_upper.npy\")\n",
    "\n",
    "gene_indices_output_path = os.path.join(output_path, \"gene_indices.npy\")\n",
    "cell_barcodes_output_path = os.path.join(output_path, \"cell_barcodes.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training schedule, optimizer, loss, and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "dtype = torch.float\n",
    "\n",
    "fsd_codec = GeneralNegativeBinomialMixtureFamilySizeDistributionCodec(\n",
    "    sc_fingerprint_datastore=sc_fingerprint_datastore,\n",
    "    n_fsd_lo_comps=1,\n",
    "    n_fsd_hi_comps=2,\n",
    "    fsd_init_params_dict=fsd_init_params_dict)\n",
    "\n",
    "model = SingleCellFamilySizeModel(\n",
    "    init_params_dict=init_params_dict,\n",
    "    model_constraint_params_dict=model_constraint_params_dict,\n",
    "    sc_fingerprint_datastore=sc_fingerprint_datastore,\n",
    "    fsd_codec=fsd_codec,\n",
    "    e_lo_prior_dist='poisson',\n",
    "    e_hi_prior_dist='zinb',\n",
    "    model_type='poisson',# 'approx_multinomial',\n",
    "    guide_type='map')\n",
    "\n",
    "optim = Adam(adam_args)\n",
    "\n",
    "custom_loss = DownsamplingRegularizedELBOLoss(\n",
    "    downsampling_regularization_strength=downsampling_regularization_strength,\n",
    "    min_downsampling_rate=min_downsampling_rate,\n",
    "    max_downsampling_rate=max_downsampling_rate,\n",
    "    disable_downsampling_regularization=disable_downsampling_regularization,\n",
    "    keep_history=True)\n",
    "\n",
    "svi = SVI(model.model, model.guide, optim, loss=custom_loss.differentiable_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mb_data = sc_fingerprint_datastore.generate_stratified_sample_torch(1, 10, 15)\n",
    "# trace = poutine.trace(model.model).get_trace(mb_data)\n",
    "# print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = []\n",
    "custom_loss.reset_history()\n",
    "# load_latest_checkpoint(checkpoint_path)\n",
    "\n",
    "# other_output_path = '/home/jupyter/data/10x/out/pbmc4k__alpha_0.0__beta_0.5__default_gene_filters__rc'\n",
    "# if other_output_path is None:\n",
    "#     model_output_path = os.path.join(output_path, f\"{dataset_name}_{sc_fingerprint_datastore.n_genes}_genes_marginalized.pyro\")\n",
    "# else:\n",
    "#     model_output_path = os.path.join(other_output_path, f\"{dataset_name}_{sc_fingerprint_datastore.n_genes}_genes_marginalized.pyro\")\n",
    "# pyro.get_param_store().load(model_output_path)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # adapt model parameters to downsampled dataset\n",
    "#     fsd_params_dict = fsd_codec.decode(pyro.param(\"fsd_xi_posterior_loc\"))\n",
    "\n",
    "#     kappa = 0.25\n",
    "#     downsampled_fsd_params_dict = dict()\n",
    "#     downsampled_fsd_params_dict['phi_lo'] = fsd_params_dict['phi_lo']\n",
    "#     downsampled_fsd_params_dict['phi_hi'] = fsd_params_dict['phi_hi']\n",
    "#     downsampled_fsd_params_dict['w_lo'] = fsd_params_dict['w_lo']\n",
    "#     downsampled_fsd_params_dict['w_hi'] = fsd_params_dict['w_hi']\n",
    "#     downsampled_fsd_params_dict['mu_lo'] = kappa * fsd_params_dict['mu_lo']\n",
    "#     downsampled_fsd_params_dict['mu_hi'] = kappa * fsd_params_dict['mu_hi']\n",
    "#     downsampled_fsd_xi_posterior_loc = fsd_codec.encode(downsampled_fsd_params_dict)\n",
    "    \n",
    "# pyro.get_param_store()['fsd_xi_posterior_loc'].data.copy_(downsampled_fsd_xi_posterior_loc.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset e_hi parameters to the initial estimate\n",
    "# from pyro_extras import logit\n",
    "# with torch.no_grad():\n",
    "#     pyro.get_param_store()._params['mu_e_hi'].data.copy_(torch.tensor(sc_fingerprint_datastore.estimated_mu_e_hi).log())\n",
    "#     pyro.get_param_store()._params['phi_e_hi'].data.copy_(torch.tensor(sc_fingerprint_datastore.estimated_phi_e_hi).log())\n",
    "#     pyro.get_param_store()._params['logit_p_zero_e_hi'].data.copy_(logit(torch.tensor(sc_fingerprint_datastore.estimated_p_zero_e_hi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.get_param_store()._params['beta_c'].data.copy_(torch.tensor(1.).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_loss_frequency = 50\n",
    "checkpoint_frequency = 1_000\n",
    "\n",
    "mb_loss_list = []\n",
    "fsd_abs_res_list = []\n",
    "loss_scale_factor = 1. / (sc_fingerprint_datastore.n_cells * sc_fingerprint_datastore.n_genes)\n",
    "prev_fsd_xi_posterior_loc = torch.zeros(\n",
    "    (sc_fingerprint_datastore.n_genes, fsd_codec.total_fsd_params),\n",
    "    dtype=dtype, device=device)\n",
    "\n",
    "t0 = time.time()\n",
    "for i_iter in range(n_iters):\n",
    "    # generate minibatch for elbo loss\n",
    "    mb_data_elbo = sc_fingerprint_datastore.generate_stratified_sample_torch(\n",
    "        mb_genes_per_gene_group,\n",
    "        mb_expressing_cells_per_gene,\n",
    "        mb_silent_cells_per_gene)\n",
    "    mb_data_elbo['e_lo_sum_width'] = training_e_lo_sum_width\n",
    "    mb_data_elbo['e_hi_sum_width'] = training_e_hi_sum_width    \n",
    "    mb_data_elbo['e_obs_log_prob_prefactor'] = training_e_obs_log_prob_prefactor\n",
    "\n",
    "    # generate minibatch for downsampling regularization\n",
    "    if not disable_downsampling_regularization:\n",
    "        mb_data_reg = sc_fingerprint_datastore.generate_stratified_sample_torch(\n",
    "            mb_genes_per_gene_group_reg,\n",
    "            mb_expressing_cells_per_gene_reg,\n",
    "            mb_silent_cells_per_gene_reg)\n",
    "        mb_data_reg['e_lo_sum_width'] = training_e_lo_sum_width\n",
    "        mb_data_reg['e_hi_sum_width'] = training_e_hi_sum_width\n",
    "        mb_data_reg['e_obs_log_prob_prefactor'] = training_e_obs_log_prob_prefactor\n",
    "    else:\n",
    "        mb_data_reg = None\n",
    "    \n",
    "    # SVI update\n",
    "    mb_loss = svi.step(mb_data_elbo, mb_data_reg)\n",
    "        \n",
    "    # bookkeeping, history tracking, etc.\n",
    "    mb_loss_per_cell_gene = loss_scale_factor * mb_loss\n",
    "    mb_loss_list.append(mb_loss_per_cell_gene)\n",
    "    \n",
    "    # calculate change in fsd parameters\n",
    "    new_fsd_xi_posterior_loc = pyro.param(\"fsd_xi_posterior_loc\").clone().detach()\n",
    "    xi_posterior_abs_res_mean = torch.mean((new_fsd_xi_posterior_loc - prev_fsd_xi_posterior_loc).abs()).item()\n",
    "    prev_fsd_xi_posterior_loc = new_fsd_xi_posterior_loc    \n",
    "    fsd_abs_res_list.append(1_000 * xi_posterior_abs_res_mean)\n",
    "    \n",
    "    if i_iter % print_loss_frequency == 0:\n",
    "        t1 = time.time()\n",
    "        \n",
    "        mb_loss_mean, mb_loss_std = np.mean(mb_loss_list), np.std(mb_loss_list)\n",
    "        fsd_abs_res_mean, fsd_abs_res_std = np.mean(fsd_abs_res_list), np.std(fsd_abs_res_list)\n",
    "        phi_e_lo_mean, phi_e_lo_std = torch.mean(pyro.param(\"phi_e_lo\")).item(), torch.std(pyro.param(\"phi_e_lo\")).item()\n",
    "        phi_e_lo_max, phi_e_lo_min = torch.max(pyro.param(\"phi_e_lo\")).item(), torch.min(pyro.param(\"phi_e_lo\")).item()\n",
    "        phi_e_hi_mean, phi_e_hi_std = torch.mean(pyro.param(\"phi_e_hi\")).item(), torch.std(pyro.param(\"phi_e_hi\")).item()\n",
    "        phi_e_hi_max, phi_e_hi_min = torch.max(pyro.param(\"phi_e_hi\")).item(), torch.min(pyro.param(\"phi_e_hi\")).item()\n",
    "        logit_p_zero_e_hi_mean, logit_p_zero_e_hi_std = (\n",
    "            torch.mean(pyro.param(\"logit_p_zero_e_hi\")).item(),\n",
    "            torch.std(pyro.param(\"logit_p_zero_e_hi\")).item())\n",
    "        logit_p_zero_e_hi_max, logit_p_zero_e_hi_min = (\n",
    "            torch.max(pyro.param(\"logit_p_zero_e_hi\")).item(),\n",
    "            torch.min(pyro.param(\"logit_p_zero_e_hi\")).item())\n",
    "\n",
    "        mb_loss_list = []\n",
    "        fsd_abs_res_list = []\n",
    "        print(f'Iteration number: {i_iter}, loss: {mb_loss_mean:.3f} +- {mb_loss_std:.3f}, ' +\n",
    "              f'fsd_abs_res: {fsd_abs_res_mean:.4f} +- {fsd_abs_res_std:.4f}, ' +\n",
    "              f'time: {(t1 - t0):.3f}s')\n",
    "        print(f'alpha_c: {pyro.param(\"alpha_c\").item():.3f}')\n",
    "        print(f'beta_c: {pyro.param(\"beta_c\").item():.3f}')\n",
    "        print(f'phi_e_lo: {phi_e_lo_mean:.4f} +- {phi_e_lo_std:.3f} [{phi_e_lo_min:.4f}, {phi_e_lo_max:.4f}]')\n",
    "        print(f'phi_e_hi: {phi_e_hi_mean:.4f} +- {phi_e_hi_std:.3f} [{phi_e_hi_min:.4f}, {phi_e_hi_max:.4f}]')\n",
    "        print(f'logit_p_zero_e_hi: {logit_p_zero_e_hi_mean:.4f} +- ' \\\n",
    "              + f'{logit_p_zero_e_hi_std:.4f} [{logit_p_zero_e_hi_min:.4f}, {logit_p_zero_e_hi_max:.4f}]')\n",
    "        t0 = t1\n",
    "        # print(f'mu_lo_global: {pyro.param(\"fsd_codec$$$log_mu_lo_global\").exp().item():.3f}')\n",
    "        # print(f'phi_lo_global: {fsd_codec.log_phi_lo_global.exp().item():.3f}')\n",
    "    loss_hist.append(mb_loss_per_cell_gene)\n",
    "\n",
    "    if i_iter % checkpoint_frequency == 0:\n",
    "        logging.warning(\"Checkpointing the latest model parameters...\")\n",
    "        checkpoint_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing = True\n",
    "window_length = 201\n",
    "polyorder = 1\n",
    "\n",
    "elbo_loss_history = custom_loss.elbo_loss_history\n",
    "downsampling_loss_history = custom_loss.downsampling_loss_history\n",
    "\n",
    "if smoothing:\n",
    "    if len(custom_loss.elbo_loss_history) > window_length:\n",
    "        elbo_loss_history = savgol_filter(\n",
    "            custom_loss.elbo_loss_history,\n",
    "            window_length=window_length,\n",
    "            polyorder=polyorder)\n",
    "    if len(custom_loss.downsampling_loss_history) > window_length:\n",
    "        downsampling_loss_history = savgol_filter(\n",
    "            custom_loss.downsampling_loss_history,\n",
    "            window_length=window_length,\n",
    "            polyorder=polyorder)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(elbo_loss_history)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(downsampling_loss_history, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fsd_codec.decode(fsd_codec.get_sorted_fsd_xi(pyro.param(\"fsd_xi_posterior_loc\")))['mu_hi'] / (\n",
    "    torch.tensor(sc_fingerprint_datastore.empirical_fsd_mu_hi).float().cuda().unsqueeze(-1))\n",
    "x = x.detach().cpu().numpy()\n",
    "plt.plot(x[:, 0], alpha=0.2, lw=0, marker='.', markersize=5)\n",
    "plt.plot(x[:, 1], alpha=0.2, lw=0, marker='.', markersize=5)\n",
    "# plt.ylim([0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_index = 1\n",
    "cell_shard_size = 200\n",
    "\n",
    "exploration_e_lo_sum_width = training_e_lo_sum_width\n",
    "exploration_e_hi_sum_width = training_e_hi_sum_width\n",
    "exploration_e_lo_log_prob_prefactor = training_e_lo_log_prob_prefactor\n",
    "exploration_e_hi_log_prob_prefactor = training_e_hi_log_prob_prefactor\n",
    "exploration_e_obs_log_prob_prefactor = training_e_obs_log_prob_prefactor\n",
    "exploration_fingerprint_obs_log_prob_prefactor = training_fingerprint_obs_log_prob_prefactor\n",
    "\n",
    "expr_dict = get_expression_map(\n",
    "    gene_index, model, sc_fingerprint_datastore,\n",
    "    e_lo_sum_width=exploration_e_lo_sum_width,\n",
    "    e_hi_sum_width=exploration_e_hi_sum_width,\n",
    "    cell_shard_size=cell_shard_size,\n",
    "    e_lo_log_prob_prefactor=exploration_e_lo_log_prob_prefactor,\n",
    "    e_hi_log_prob_prefactor=exploration_e_hi_log_prob_prefactor,\n",
    "    e_obs_log_prob_prefactor=exploration_e_obs_log_prob_prefactor,\n",
    "    fingerprint_obs_log_prob_prefactor=exploration_fingerprint_obs_log_prob_prefactor)\n",
    "\n",
    "e_lo, e_hi = expr_dict['e_lo_map'], expr_dict['e_hi_map']\n",
    "\n",
    "fsd_xi_loc = pyro.param(\"fsd_xi_posterior_loc\")[gene_index, :]\n",
    "fsd_params_dict = fsd_codec.decode(fsd_xi_loc.unsqueeze(0))\n",
    "dist_lo, dist_hi = fsd_codec.get_fsd_components(fsd_params_dict, downsampling_rate_tensor=None)\n",
    "\n",
    "assert model.e_hi_prior_dist == 'zinb'\n",
    "e_hi_prior_dist = ZeroInflatedNegativeBinomial(\n",
    "    logit_zero=pyro.param(\"logit_p_zero_e_hi\"),\n",
    "    mu=pyro.param(\"mu_e_hi\"),\n",
    "    phi=pyro.param(\"phi_e_hi\"))\n",
    "\n",
    "\n",
    "e_obs = np.sum(sc_fingerprint_datastore.fingerprint_array[:, gene_index, :], -1)\n",
    "mean_e_obs = np.mean(e_obs)\n",
    "mean_e_hi = np.mean(e_hi)\n",
    "mean_e_lo = np.mean(e_lo)\n",
    "e_hi_z = np.sum(e_hi == 0)\n",
    "e_obs_z = np.sum(e_obs == 0)\n",
    "\n",
    "print('mean_e_obs:', mean_e_obs)\n",
    "print('mean_e_hi (map):', mean_e_hi)\n",
    "print('mean_e_lo (map):', mean_e_lo)\n",
    "print('e_hi_z:', e_hi_z)\n",
    "print('e_obs_z:', e_obs_z)\n",
    "print('mu_e_hi (prior):', pyro.param(\"mu_e_hi\")[gene_index].item())\n",
    "print('phi_e_hi (prior):', pyro.param(\"phi_e_hi\")[gene_index].item())\n",
    "print('p_zero_e_hi (prior):', torch.sigmoid(pyro.param(\"logit_p_zero_e_hi\")[gene_index]).item())\n",
    "print('mean_e_hi (prior):', e_hi_prior_dist.mean[gene_index].item())\n",
    "print('phi_e_lo:', pyro.param(\"phi_e_lo\")[gene_index].item())\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "e_obs = np.sum(sc_fingerprint_datastore.fingerprint_array[:, gene_index, :], -1)\n",
    "plt.scatter(e_obs, e_hi, s=1, alpha=0.5)\n",
    "plt.plot([0.1, np.max(e_obs)], [0.1, np.max(e_obs)])\n",
    "plt.xlabel('e_obs', fontsize=16)\n",
    "plt.ylabel('e_hi', fontsize=16)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim((0.1, 2*np.max(e_obs)))\n",
    "plt.ylim((0.1, 2*np.max(e_obs)))\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(e_obs, expr_dict['mu_e_lo'], alpha=0.5, s=1, label='prior')\n",
    "plt.scatter(e_obs, e_lo, alpha=0.5, s=1, label='posterior')\n",
    "plt.plot([0.1, np.max(e_obs)], [0.1, np.max(e_obs)])\n",
    "plt.xlabel('e_obs', fontsize=16)\n",
    "plt.ylabel('e_lo', fontsize=16)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim((0.1, 2*np.max(e_obs)))\n",
    "plt.ylim((0.1, 2*np.max(e_obs)))\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "_ = plt.hist(e_lo - expr_dict['e_lo_min'], alpha=0.5, range=(0, exploration_e_lo_sum_width), bins=exploration_e_lo_sum_width)\n",
    "_ = plt.hist(e_hi - expr_dict['e_hi_min'], alpha=0.5, range=(0, exploration_e_hi_sum_width), bins=exploration_e_hi_sum_width)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for k, v in fsd_params_dict.items():\n",
    "    print(f\"{k}: {v.clone().detach().cpu()}\")\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "p_hi = dist_hi.log_prob(torch.arange(0, sc_fingerprint_datastore.max_family_size + 1).float().cuda()).detach().cpu().exp().numpy().flatten()\n",
    "plt.plot(p_hi)\n",
    "\n",
    "fig = plt.figure()\n",
    "p_lo = dist_lo.log_prob(torch.arange(0, sc_fingerprint_datastore.max_family_size + 1).float().cuda()).detach().cpu().exp().numpy().flatten()\n",
    "plt.plot(p_lo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "empircal_fsd = sc_fingerprint_datastore.fingerprint_array[:, gene_index, :]\n",
    "empircal_fsd = empircal_fsd / (1e-12 + np.sum(empircal_fsd, -1)[:, None])\n",
    "empircal_fsd = np.sum(empircal_fsd, 0)\n",
    "empircal_fsd = empircal_fsd / np.sum(empircal_fsd)\n",
    "\n",
    "model_fsd = e_obs[:, None] * np.exp(expr_dict['fit_log_prob_map'])\n",
    "model_fsd = model_fsd / (1e-12 + np.sum(model_fsd, -1)[:, None])\n",
    "model_fsd = np.sum(model_fsd, 0)\n",
    "model_fsd = model_fsd / np.sum(model_fsd)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(empircal_fsd, label='empirical')\n",
    "plt.plot(model_fsd, label='model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,4))\n",
    "\n",
    "err = np.vstack((expr_dict['e_hi_map'] - expr_dict['e_hi_ci_lower'], expr_dict['e_hi_ci_upper'] - expr_dict['e_hi_map']))\n",
    "# err = np.sqrt(expr_dict['e_hi_var'])\n",
    "plt.scatter(np.arange(0, sc_fingerprint_datastore.n_cells), e_obs, marker='o', color='cyan', s=30)\n",
    "plt.errorbar(np.arange(0, sc_fingerprint_datastore.n_cells),\n",
    "             expr_dict['e_hi_map'],\n",
    "             barsabove=True, elinewidth=1, lw=0,\n",
    "             marker='.', ecolor='gray', markersize=8,\n",
    "             yerr=err, alpha=0.5)\n",
    "# plt.scatter(np.arange(0, sc_fingerprint_datastore.n_cells), expr_dict['e_hi_mean'], marker='x', color='red', s=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "x_vals = e_obs\n",
    "y_vals = expr_dict['e_hi_map']\n",
    "combos = list(zip(x_vals, y_vals))\n",
    "weight_counter = Counter(combos)\n",
    "weights = [weight_counter[(x_vals[i], y_vals[i])] for i, _ in enumerate(x_vals)]\n",
    "\n",
    "\n",
    "plt.scatter(x_vals, y_vals, s=weights, alpha=0.7)\n",
    "plt.plot([0, np.max(x_vals)], [0, np.max(x_vals)], '--', color='black')\n",
    "plt.ylabel(r'$e^>$', fontsize=16)\n",
    "plt.xlabel(r'$e_\\mathrm{obs}$', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fsd_params_dict = fsd_codec.decode(pyro.param(\"fsd_xi_posterior_loc\"))\n",
    "    dist_lo, dist_hi = fsd_codec.get_fsd_components(fsd_params_dict, downsampling_rate_tensor=None)\n",
    "    mu_lo = dist_lo.mean.cpu().numpy()\n",
    "    mu_hi = dist_hi.mean.cpu().numpy()\n",
    "    phi_lo = fsd_params_dict['phi_lo'].squeeze(-1).cpu().numpy()\n",
    "    \n",
    "assignments = torch.argmin(\n",
    "    (fsd_codec.get_sorted_fsd_xi(pyro.param(\"fsd_xi_posterior_loc\")).unsqueeze(-1)\n",
    "     - fsd_codec.get_sorted_fsd_xi(pyro.param(\"fsd_xi_prior_locs\")).permute(-1, -2)).pow(2).sum(1), dim=-1).detach().cpu().numpy()\n",
    "\n",
    "plt.scatter(mu_hi.squeeze(), mu_lo.squeeze() / mu_hi.squeeze(), s=1, alpha=0.5, c=assignments, cmap=plt.cm.jet)\n",
    "plt.scatter(mu_hi.squeeze(), phi_lo.squeeze(), s=1, alpha=0.5, c=assignments, cmap=plt.cm.jet)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.ylim((1e-5, 10.))\n",
    "plt.xlim((0.1, 50)) \n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(assignments == 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_p_zero_e_hi = pyro.param(\"logit_p_zero_e_hi\").detach().cpu().numpy()\n",
    "_ = plt.hist(logit_p_zero_e_hi, bins=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = os.path.join(output_path, f\"{dataset_name}_{sc_fingerprint_datastore.n_genes}_genes_marginalized.pyro\")\n",
    "pyro.get_param_store().save(model_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and save the droplet-count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro_extras import ZeroInflatedNegativeBinomial\n",
    "\n",
    "log_frequency = 5\n",
    "max_cells_per_e_lo_e_hi = 500_000\n",
    "max_adaptive_extraction_e_lo_sum_width = 100\n",
    "max_adaptive_extraction_e_hi_sum_width = 100\n",
    "\n",
    "extraction_e_lo_sum_width = training_e_lo_sum_width\n",
    "extraction_e_hi_sum_width = training_e_hi_sum_width\n",
    "extraction_e_lo_log_prob_prefactor = training_e_lo_log_prob_prefactor\n",
    "extraction_e_hi_log_prob_prefactor = training_e_hi_log_prob_prefactor\n",
    "extraction_e_obs_log_prob_prefactor = training_e_obs_log_prob_prefactor\n",
    "extraction_fingerprint_obs_log_prob_prefactor = training_fingerprint_obs_log_prob_prefactor\n",
    "\n",
    "# calculate some auxiliary quantities for adaptive determination of marginalization widths\n",
    "assert model.e_hi_prior_dist == 'zinb'\n",
    "e_hi_prior_dist = ZeroInflatedNegativeBinomial(\n",
    "    logit_zero=pyro.param(\"logit_p_zero_e_hi\"),\n",
    "    mu=pyro.param(\"mu_e_hi\"),\n",
    "    phi=pyro.param(\"phi_e_hi\"))\n",
    "e_hi_std = e_hi_prior_dist.variance.sqrt().clone().detach().cpu().numpy()\n",
    "e_hi_mean = e_hi_prior_dist.mean.clone().detach().cpu().numpy()\n",
    "\n",
    "fsd_dist_lo, fsd_dist_hi = fsd_codec.get_fsd_components(fsd_codec.decode(pyro.param(\"fsd_xi_posterior_loc\")), None)\n",
    "fsd_mu_hi = fsd_dist_hi.mean.clone().detach().cpu().numpy().squeeze()\n",
    "e_hi_mu = pyro.param(\"mu_e_hi\").clone().detach().cpu().numpy()\n",
    "e_lo_std = (pyro.param(\"alpha_c\").item() + pyro.param(\"beta_c\").item()) * (\n",
    "    e_hi_mean * fsd_mu_hi / model.median_fsd_mu_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_lo_map = np.zeros((sc_fingerprint_datastore.n_cells, sc_fingerprint_datastore.n_genes), dtype=np.int)\n",
    "e_lo_mean = np.zeros((sc_fingerprint_datastore.n_cells, sc_fingerprint_datastore.n_genes), dtype=np.float32)\n",
    "e_lo_var = np.zeros((sc_fingerprint_datastore.n_cells, sc_fingerprint_datastore.n_genes), dtype=np.float32)\n",
    "e_lo_ci_lower = np.zeros((sc_fingerprint_datastore.n_cells, sc_fingerprint_datastore.n_genes), dtype=np.int)\n",
    "e_lo_ci_upper = np.zeros((sc_fingerprint_datastore.n_cells, sc_fingerprint_datastore.n_genes), dtype=np.int)\n",
    "\n",
    "e_hi_map = np.zeros((sc_fingerprint_datastore.n_cells, sc_fingerprint_datastore.n_genes), dtype=np.int)\n",
    "e_hi_mean = np.zeros((sc_fingerprint_datastore.n_cells, sc_fingerprint_datastore.n_genes), dtype=np.float32)\n",
    "e_hi_var = np.zeros((sc_fingerprint_datastore.n_cells, sc_fingerprint_datastore.n_genes), dtype=np.float32)\n",
    "e_hi_ci_lower = np.zeros((sc_fingerprint_datastore.n_cells, sc_fingerprint_datastore.n_genes), dtype=np.int)\n",
    "e_hi_ci_upper = np.zeros((sc_fingerprint_datastore.n_cells, sc_fingerprint_datastore.n_genes), dtype=np.int)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i_gene in range(sc_fingerprint_datastore.n_genes):\n",
    "    adaptive_extraction_e_lo_sum_width = min(\n",
    "        max_adaptive_extraction_e_lo_sum_width,\n",
    "        extraction_e_lo_sum_width + int(np.ceil(2 * e_lo_std[i_gene])))\n",
    "    adaptive_extraction_e_hi_sum_width = min(\n",
    "        max_adaptive_extraction_e_hi_sum_width,\n",
    "        extraction_e_hi_sum_width + int(np.ceil(2 * e_hi_std[i_gene])))\n",
    "    adaptive_cell_shard_size = min(\n",
    "        sc_fingerprint_datastore.n_cells,\n",
    "        max(1, int(np.ceil(max_cells_per_e_lo_e_hi / (\n",
    "            adaptive_extraction_e_lo_sum_width * adaptive_extraction_e_hi_sum_width)))))\n",
    "    \n",
    "    expr_dict = get_expression_map(\n",
    "        i_gene, model, sc_fingerprint_datastore,\n",
    "        e_lo_sum_width=adaptive_extraction_e_lo_sum_width,\n",
    "        e_hi_sum_width=adaptive_extraction_e_hi_sum_width,\n",
    "        cell_shard_size=adaptive_cell_shard_size,\n",
    "        e_lo_log_prob_prefactor=extraction_e_lo_log_prob_prefactor,\n",
    "        e_hi_log_prob_prefactor=extraction_e_hi_log_prob_prefactor,\n",
    "        e_obs_log_prob_prefactor=extraction_e_obs_log_prob_prefactor,\n",
    "        fingerprint_obs_log_prob_prefactor=extraction_fingerprint_obs_log_prob_prefactor)\n",
    "    \n",
    "    e_lo_map[:, i_gene] = expr_dict['e_lo_map']\n",
    "    e_lo_mean[:, i_gene] = expr_dict['e_lo_mean']\n",
    "    e_lo_var[:, i_gene] = expr_dict['e_lo_var']\n",
    "    e_lo_ci_lower[:, i_gene] = expr_dict['e_lo_ci_lower']\n",
    "    e_lo_ci_upper[:, i_gene] = expr_dict['e_lo_ci_upper']\n",
    "\n",
    "    e_hi_map[:, i_gene] = expr_dict['e_hi_map']\n",
    "    e_hi_mean[:, i_gene] = expr_dict['e_hi_mean']\n",
    "    e_hi_var[:, i_gene] = expr_dict['e_hi_var']\n",
    "    e_hi_ci_lower[:, i_gene] = expr_dict['e_hi_ci_lower']\n",
    "    e_hi_ci_upper[:, i_gene] = expr_dict['e_hi_ci_upper']\n",
    "    \n",
    "    if i_gene % log_frequency == 0 and i_gene > 0:\n",
    "        t1 = time.time()\n",
    "        seconds_per_gene = (t1 - t0) / log_frequency\n",
    "        t0 = t1\n",
    "        logging.warning(f'Processing gene {i_gene+1}/{sc_fingerprint_datastore.n_genes} ({seconds_per_gene:.2f} s/gene)...')\n",
    "\n",
    "# gene indices and cell barcodes\n",
    "original_gene_indices = np.asarray(list(map(\n",
    "    sc_fingerprint_datastore.internal_gene_index_to_original_gene_index_map.get,\n",
    "    range(sc_fingerprint_datastore.n_genes))))\n",
    "cell_barcodes = np.asarray(sc_fingerprint.barcode_list)\n",
    "\n",
    "# save\n",
    "np.save(e_lo_map_output_path, e_lo_map)\n",
    "np.save(e_lo_mean_output_path, e_lo_mean)\n",
    "np.save(e_lo_var_output_path, e_lo_var)\n",
    "np.save(e_lo_ci_lower_output_path, e_lo_ci_lower)\n",
    "np.save(e_lo_ci_upper_output_path, e_lo_ci_upper)\n",
    "\n",
    "np.save(e_hi_map_output_path, e_hi_map)\n",
    "np.save(e_hi_mean_output_path, e_hi_mean)\n",
    "np.save(e_hi_var_output_path, e_hi_var)\n",
    "np.save(e_hi_ci_lower_output_path, e_hi_ci_lower)\n",
    "np.save(e_hi_ci_upper_output_path, e_hi_ci_upper)\n",
    "np.save(gene_indices_output_path, original_gene_indices)\n",
    "np.save(cell_barcodes_output_path, cell_barcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the distributions parameters $\\xi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "from pyro_extras import MixtureDistribution\n",
    "\n",
    "with torch.no_grad():\n",
    "    xi_posterior = fsd_codec.get_sorted_fsd_xi(pyro.param(\"fsd_xi_posterior_loc\")).detach().cpu().numpy()\n",
    "\n",
    "    n_prior_samples = 100_000\n",
    "    fsd_xi_prior_locs = fsd_codec.get_sorted_fsd_xi(pyro.param(\"fsd_xi_prior_locs\"))\n",
    "    fsd_xi_prior_scales = pyro.param(\"fsd_xi_prior_scales\")\n",
    "    \n",
    "    if model.fsd_gmm_num_components > 1:\n",
    "        fsd_xi_prior_log_weights = pyro.param(\"fsd_xi_prior_weights_map\").log()\n",
    "        fsd_xi_prior_log_weights_tuple = tuple(\n",
    "            fsd_xi_prior_log_weights[j]\n",
    "            for j in range(model.fsd_gmm_num_components))        \n",
    "        fsd_xi_prior_components_tuple = tuple(\n",
    "            dist.Normal(fsd_xi_prior_locs[j, :], fsd_xi_prior_scales[j, :]).to_event(1)\n",
    "            for j in range(model.fsd_gmm_num_components))\n",
    "        fsd_xi_prior_dist = MixtureDistribution(\n",
    "            fsd_xi_prior_log_weights_tuple, fsd_xi_prior_components_tuple)\n",
    "    else:\n",
    "            fsd_xi_prior_dist = dist.Normal(fsd_xi_prior_locs[0, :], fsd_xi_prior_scales[0, :]).to_event(1)\n",
    "\n",
    "\n",
    "    xi_prior = fsd_xi_prior_dist.sample((n_prior_samples,)).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "xi_prior_pca_fit = pca.fit(xi_prior)\n",
    "xi_prior_pca = xi_prior_pca_fit.transform(xi_prior)\n",
    "xi_posterior_pca = xi_prior_pca_fit.transform(xi_posterior)\n",
    "xi_prior_locs_pca = xi_prior_pca_fit.transform(pyro.param(\"fsd_xi_prior_locs\").detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.scatter(xi_prior_pca[:, 0], xi_prior_pca[:, 1], alpha=0.1, s=1)\n",
    "plt.scatter(xi_posterior_pca[:, 0], xi_posterior_pca[:, 1], alpha=0.7, s=2, color='red')\n",
    "for j in range(model.fsd_gmm_num_components):\n",
    "    plt.scatter(xi_prior_locs_pca[j, 0], xi_prior_locs_pca[j, 1], s=30, marker='o', label=str(j))\n",
    "    plt.text(xi_prior_locs_pca[j, 0], xi_prior_locs_pca[j, 1], str(j), color='cyan', size=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "for fsd_gmm_comp_idx in range(5):\n",
    "    fsd_xi = pyro.param(\"fsd_xi_prior_locs\")[fsd_gmm_comp_idx, :]\n",
    "    dist_lo, dist_hi = fsd_codec.get_fsd_components(fsd_codec.decode(fsd_xi), None)\n",
    "    max_fs = sc_fingerprint_datastore.max_family_size + 1\n",
    "\n",
    "    p_hi = dist_hi.log_prob(torch.arange(1, max_fs).float().cuda()).detach().cpu().exp().numpy().flatten()\n",
    "    p_lo = dist_lo.log_prob(torch.arange(1, max_fs).float().cuda()).detach().cpu().exp().numpy().flatten()\n",
    "    plt.plot(p_hi, color='blue', alpha=np.sqrt(pyro.param(\"fsd_xi_prior_weights_map\")[fsd_gmm_comp_idx].item()))\n",
    "    plt.plot(p_lo, color='red', alpha=np.sqrt(pyro.param(\"fsd_xi_prior_weights_map\")[fsd_gmm_comp_idx].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.param(\"fsd_xi_prior_locs\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsd_codec.decode(pyro.param(\"fsd_xi_prior_locs\")[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsd_codec.decode(pyro.param(\"fsd_xi_prior_locs\")[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsd_codec.decode(pyro.param(\"fsd_xi_prior_locs\")[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "def get_non_doublet_indices(counts_matrix):\n",
    "    return list(map(operator.itemgetter(0),\n",
    "             sorted(enumerate(np.sum(counts_matrix, -1)),\n",
    "                    key=operator.itemgetter(1), reverse=True)))[(counts_matrix.shape[0] // 10):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_trans = umap.UMAP(min_dist=0.1, metric='correlation', n_neighbors=30, random_state=1984)\n",
    "top_k_genes = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized droplet counts\n",
    "normalized_droplet_count = droplet_counts[:, :top_k_genes] / np.sum(droplet_counts[:, :top_k_genes], -1)[:, None]\n",
    "\n",
    "# normalized obs (raw) counts\n",
    "obs_counts = np.sum(sc_fingerprint_datastore.fingerprint_array, -1)\n",
    "normalized_obs_count = obs_counts[:, :top_k_genes] / np.sum(obs_counts[:, :top_k_genes], -1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_indices = get_non_doublet_indices(obs_counts)\n",
    "obs_counts_embedding = umap_trans.fit_transform(normalized_obs_count[kept_indices, :])\n",
    "plt.scatter(obs_counts_embedding[:, 0], obs_counts_embedding[:, 1], s=1, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_indices = get_non_doublet_indices(droplet_counts)\n",
    "droplet_counts_embedding = umap_trans.fit_transform(normalized_droplet_count[kept_indices, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(droplet_counts_embedding[:, 0], droplet_counts_embedding[:, 1], s=1, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# demonstration that log det of sorting operation = 0, even with stickbreaking parameters\n",
    "#\n",
    "# fsd_codec = GeneralNegativeBinomialMixtureFamilySizeDistributionCodec(\n",
    "#     sc_fingerprint_datastore=sc_fingerprint_datastore,\n",
    "#     n_fsd_lo_comps=3,\n",
    "#     n_fsd_hi_comps=4,\n",
    "#     fsd_init_params_dict=fsd_init_params_dict)\n",
    "\n",
    "# xi = torch.nn.Parameter(torch.randn(fsd_codec.total_fsd_params, device=fsd_codec.device), requires_grad=True)\n",
    "# xi_sorted = fsd_codec.get_sorted_fsd_xi(xi)\n",
    "# grads = []\n",
    "# for j in range(len(xi)):\n",
    "#     xi_sorted[j].backward(retain_graph=True)\n",
    "#     grads.append(xi.grad.clone())\n",
    "#     _ = xi.grad.data.zero_()\n",
    "\n",
    "# torch.det(torch.stack(grads))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
